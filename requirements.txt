# AnyMAL: Multimodal LLM Replication
# Based on arXiv:2309.16058

# Core PyTorch
torch>=2.0.0
torchvision>=0.15.0
torchaudio>=2.0.0

# Transformers and PEFT
transformers>=4.36.0
accelerate>=0.25.0
peft>=0.7.0
bitsandbytes>=0.41.0

# Flash Attention (optional - requires CUDA toolkit and compatible GPU)
# Uncomment if you have CUDA toolkit installed:
# flash-attn>=2.3.0

# Vision models
open_clip_torch>=2.23.0
timm>=0.9.0

# Data loading
datasets>=2.14.0
webdataset>=0.2.0
huggingface_hub>=0.19.0

# Image processing
Pillow>=10.0.0
opencv-python>=4.8.0

# Logging and monitoring
wandb>=0.16.0
tensorboard>=2.14.0

# Evaluation
pycocoevalcap>=1.2
nltk>=3.8.0

# Utilities
numpy>=1.24.0
scipy>=1.11.0
tqdm>=4.66.0
pyyaml>=6.0.0
omegaconf>=2.3.0
einops>=0.7.0

# Development
pytest>=7.4.0
black>=23.0.0
isort>=5.12.0
