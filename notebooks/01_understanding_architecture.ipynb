{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the AnyMAL Architecture\n",
    "\n",
    "This notebook provides a deep dive into how AnyMAL converts images into tokens that a language model can understand.\n",
    "\n",
    "## Overview\n",
    "\n",
    "AnyMAL follows a simple but powerful paradigm:\n",
    "1. **Encode** images with a frozen vision encoder (CLIP)\n",
    "2. **Project** vision features to the LLM's embedding space\n",
    "3. **Generate** text using the frozen LLM\n",
    "\n",
    "The key insight is that we only need to train the projection layer - everything else can stay frozen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For visualization\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Image Encoding with CLIP\n",
    "\n",
    "CLIP's Vision Transformer (ViT) converts an image into a sequence of patch embeddings.\n",
    "\n",
    "For ViT-L/14:\n",
    "- Input: 224×224 RGB image\n",
    "- Patches: 14×14 pixels each\n",
    "- Number of patches: (224/14)² = 256\n",
    "- Plus 1 CLS token = 257 total tokens\n",
    "- Hidden dimension: 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.encoders.image_encoder import ImageEncoder\n",
    "\n",
    "# Create encoder (downloads weights on first use)\n",
    "encoder = ImageEncoder(\n",
    "    model_name=\"ViT-L-14\",\n",
    "    pretrained=\"openai\",\n",
    "    freeze=True,\n",
    ")\n",
    "\n",
    "print(f\"Output dimension: {encoder.get_output_dim()}\")\n",
    "print(f\"Number of patches: {encoder.get_num_patches()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a sample image\n",
    "from data.data_utils import get_image_transform\n",
    "\n",
    "transform = get_image_transform(image_size=224, is_train=False)\n",
    "\n",
    "# Create a random test image\n",
    "import numpy as np\n",
    "test_image = Image.fromarray(np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8))\n",
    "image_tensor = transform(test_image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "print(f\"Input shape: {image_tensor.shape}\")\n",
    "\n",
    "# Encode\n",
    "with torch.no_grad():\n",
    "    features = encoder(image_tensor)\n",
    "\n",
    "print(f\"Output shape: {features.shape}\")\n",
    "print(f\"  - Batch size: {features.shape[0]}\")\n",
    "print(f\"  - Sequence length: {features.shape[1]} (256 patches + 1 CLS)\")\n",
    "print(f\"  - Hidden dim: {features.shape[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Projecting to LLM Space\n",
    "\n",
    "The Perceiver Resampler:\n",
    "1. Takes 257 tokens from CLIP (dim=1024)\n",
    "2. Compresses to 64 tokens (dim=4096)\n",
    "\n",
    "This is done via cross-attention with learnable queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.projectors.perceiver_resampler import PerceiverResampler\n",
    "\n",
    "projector = PerceiverResampler(\n",
    "    input_dim=1024,    # CLIP output dim\n",
    "    output_dim=4096,   # LLaMA hidden dim\n",
    "    num_latents=64,    # Output tokens\n",
    "    num_layers=6,\n",
    ")\n",
    "\n",
    "print(f\"Projector parameters: {projector.get_num_params():,}\")\n",
    "\n",
    "# Project features\n",
    "image_tokens = projector(features)\n",
    "print(f\"\\nProjection: {features.shape} -> {image_tokens.shape}\")\n",
    "print(f\"  - Compressed from 257 to 64 tokens\")\n",
    "print(f\"  - Dimension changed from 1024 to 4096\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: The Full Forward Pass\n",
    "\n",
    "The complete AnyMAL forward pass:\n",
    "1. Encode image → [B, 257, 1024]\n",
    "2. Project → [B, 64, 4096]\n",
    "3. Embed text → [B, text_len, 4096]\n",
    "4. Concatenate → [B, 64 + text_len, 4096]\n",
    "5. LLM forward → logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the architecture\n",
    "print(\"\"\"\n",
    "AnyMAL Architecture\n",
    "══════════════════════════════════════════════════════════════\n",
    "\n",
    "                        Input Image\n",
    "                        (224 × 224)\n",
    "                             │\n",
    "                             ▼\n",
    "                ┌────────────────────────┐\n",
    "                │    CLIP ViT-L/14       │  ← FROZEN\n",
    "                │    (Vision Encoder)    │\n",
    "                └────────────────────────┘\n",
    "                             │\n",
    "                    [B, 257, 1024]\n",
    "                             │\n",
    "                             ▼\n",
    "                ┌────────────────────────┐\n",
    "                │  Perceiver Resampler   │  ← TRAINABLE\n",
    "                │   (Cross-Attention)    │\n",
    "                └────────────────────────┘\n",
    "                             │\n",
    "                     [B, 64, 4096]\n",
    "                             │\n",
    "        ┌────────────────────┴────────────────────┐\n",
    "        │                                         │\n",
    "        ▼                                         ▼\n",
    "   Image Tokens                             Text Tokens\n",
    "   [B, 64, 4096]                           [B, T, 4096]\n",
    "        │                                         │\n",
    "        └───────────────┬─────────────────────────┘\n",
    "                        │\n",
    "                        ▼\n",
    "               Concatenate along seq\n",
    "                [B, 64+T, 4096]\n",
    "                        │\n",
    "                        ▼\n",
    "                ┌────────────────────────┐\n",
    "                │     LLaMA-3 8B         │  ← FROZEN (+ LoRA in Stage 2)\n",
    "                │   (Language Model)     │\n",
    "                └────────────────────────┘\n",
    "                        │\n",
    "                        ▼\n",
    "                     Logits\n",
    "                 [B, 64+T, vocab]\n",
    "                        │\n",
    "                        ▼\n",
    "             Cross-Entropy Loss\n",
    "            (on text portion only)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "\n",
    "### Why freeze the encoders?\n",
    "- CLIP already has excellent visual representations\n",
    "- LLaMA already understands language well\n",
    "- We just need to bridge the gap between them\n",
    "\n",
    "### Why use cross-attention projection?\n",
    "- Learnable compression of visual information\n",
    "- Fixed output size regardless of input\n",
    "- Each latent can specialize in different image aspects\n",
    "\n",
    "### Why 64 tokens?\n",
    "- Good balance between information and efficiency\n",
    "- 257 tokens would be expensive for the LLM\n",
    "- Too few tokens might lose important details"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
