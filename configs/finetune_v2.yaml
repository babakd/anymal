# AnyMALv2 Stage 2: Instruction Tuning Configuration

defaults:
  - base

stage: 2
stage_name: "instruction_tuning_v2"

model:
  architecture: "anymal_v2"
  llm_model_name: "./checkpoints/llama3-8b-instruct"
  use_qlora: true
  lora_r: 64
  lora_alpha: 16
  lora_dropout: 0.05
  lora_target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  use_flash_attention: true
  gradient_checkpointing: true

  vision_encoder_type: "siglip2"
  vision_model_name: "google/siglip2-so400m-patch14-384"
  token_compressor_type: "learned"
  bottleneck_dim: 2048
  max_image_tokens: 384
  min_image_tokens: 384

data:
  train_data_path: "./data/llava_instruct_150k.json"
  image_dir: "./data/coco/train2017"
  eval_data_path: null
  image_size: 224
  max_length: 2304
  image_token_policy: "fixed"
  per_device_batch_size: 6
  gradient_accumulation_steps: 2
  dataloader_num_workers: 4
  pin_memory: true

training:
  learning_rate: 1.0e-5
  lr_scheduler_type: "cosine"
  warmup_steps: 100
  weight_decay: 0.01
  max_grad_norm: 1.0
  max_steps: 3000
  num_epochs: null
  train_projector_only: false
  use_amp: true
  amp_dtype: "bfloat16"
  seed: 42

checkpoint:
  pretrain_checkpoint: "./outputs/pretrain_v2_alignment/checkpoint-20000"
  load_projector: true
  load_lora: false

logging:
  logging_steps: 50
  use_wandb: true
  wandb_project: "anymal-finetune-v2"
  wandb_run_name: "stage2-v2"

checkpointing:
  save_steps: 500
  save_total_limit: 3
  output_dir: "./outputs/finetune_v2"

evaluation:
  eval_steps: 500
  eval_strategy: "steps"
